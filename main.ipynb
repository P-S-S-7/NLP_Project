{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project round -1 below\n",
    "# Import necessary modules and functions from all the other modules\n",
    "from pre_processing import runPreprocessing\n",
    "from tokenization import runTokenization\n",
    "from stop_words_removal import removeStopWords\n",
    "from freq_distribution_of_tokens import GetFrequencyDistribution, VisualiseFrequencyDistribution, CreateWordCloud\n",
    "from pos_tagging import PerformPOSTagging, GetFrequencyDistributionOfTags\n",
    "import random\n",
    "from bigram_modelling import generateBigrams, CreateBigramProbabilityTable\n",
    "from shannon_game import CreateBlanksForShannonGame, PlayingTheShannonGame\n",
    "from tf_idf_vectors import GenerateTF_IDF_Vectors\n",
    "from chapter_similarity import ComputeSimilarityBetweenChapters\n",
    "from similarity_visualisation import VisualiseSimilarity\n",
    "from named_entity_recognition import performNERonEntireText, extractNERTagsFromRandomPassage\n",
    "from performance_evaluation_and_visualisation import evaluatePerformance, visualisePerformanceUsingBarPlots, visualisePerformanceUsingConfusionMatrix\n",
    "\n",
    "# Import all the content of the selected book and open it in read mode \n",
    "orignalFile = open('Pride_and_Prejudice.txt', 'r')\n",
    "content = orignalFile.read()\n",
    "\n",
    "# Run preprocessing on the content to prepare NLP ready data\n",
    "preprocessedTextChapters = runPreprocessing(content)\n",
    "\n",
    "# Combine preprocessed chapters into a single string data structure\n",
    "preprocessedText= \"\"\n",
    "for i in preprocessedTextChapters:\n",
    "    preprocessedText = preprocessedText + i + \"\\n\"\n",
    "\n",
    "# Tokenize the preprocessed text \n",
    "tokenizedText = runTokenization(preprocessedText)\n",
    "\n",
    "# Remove stopwords from the tokenized text\n",
    "tokenisedTextWithoutStopwords = removeStopWords(tokenizedText)\n",
    "\n",
    "# Print and visualize the frequency distribution of tokens using BarGraphs and Frequency Distributions\n",
    "tokenFD = GetFrequencyDistribution(tokenisedTextWithoutStopwords)\n",
    "VisualiseFrequencyDistribution(tokenFD)\n",
    "\n",
    "# Create a word cloud from the tokenized text to visualise its frequency distribution\n",
    "CreateWordCloud(tokenisedTextWithoutStopwords)\n",
    "\n",
    "# Perform Part-of-Speech (POS) tagging on the tokenized text using the default penn-treebank tagset \n",
    "taggedTokens = PerformPOSTagging(tokenisedTextWithoutStopwords)\n",
    "\n",
    "#Analysing the frequency distrinution of the most frequent tags using plots\n",
    "GetFrequencyDistributionOfTags(taggedTokens)\n",
    "\n",
    "# Randomly select 10 chapters for the bigram model training set and merge them into a sinlge string of text\n",
    "randomChapterIndexes = random.sample(range(1, len(preprocessedTextChapters)), 10)\n",
    "trainingSetForBigramModel = \"\"\n",
    "for chapterNumber in randomChapterIndexes:\n",
    "    trainingSetForBigramModel += preprocessedTextChapters[chapterNumber]\n",
    "\n",
    "# Tokenize the training set for bigram modeling without removing the stop words \n",
    "trainingSetForBigramModel = runTokenization(trainingSetForBigramModel)\n",
    "\n",
    "# Generate bigrams from the training set\n",
    "bigrams = generateBigrams(trainingSetForBigramModel)\n",
    "\n",
    "# Create and print a bigram probability table\n",
    "bigramProbabiltyTable = CreateBigramProbabilityTable(bigrams, trainingSetForBigramModel)\n",
    "\n",
    "# Extract distinct tokens from the training set useful in the shannon game module\n",
    "distinct_tokens = list(set(sorted(trainingSetForBigramModel)))\n",
    "\n",
    "preprocessedTextChaptersForTesting = []\n",
    "# Iterate through chapters and add chapters not in the training set to the list\n",
    "for chapterNumber in range(len(preprocessedTextChapters)):\n",
    "    if chapterNumber not in randomChapterIndexes:\n",
    "        preprocessedTextChaptersForTesting.append(preprocessedTextChapters[chapterNumber])\n",
    "\n",
    "# Randomly select a test chapter from the chapters not used in training\n",
    "testPreprocessedTextChapter = random.choice(preprocessedTextChaptersForTesting)\n",
    "\n",
    "# Tokenize the test chapter for the Shannon game\n",
    "testTokenSetForShannonGame = runTokenization(testPreprocessedTextChapter)\n",
    "\n",
    "# Create blanks in the test token set for the Shannon game and store original tokens that the blanks replaced\n",
    "testTokenSetWithBlanks, originalTokens = CreateBlanksForShannonGame(testTokenSetForShannonGame, 50)\n",
    "\n",
    "# Play the Shannon game with the test token set and get the accuracy of the bigram model trained above\n",
    "PlayingTheShannonGame(testTokenSetWithBlanks, bigramProbabiltyTable, 50, distinct_tokens, originalTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round-2 code below\n",
    "#Implementing the part-2 of round-2 first\n",
    "\n",
    "#part-2 below\n",
    "# Tokenizing all the chapters to prepare them for the tf_idf vectorization\n",
    "# although the tf_idf vectorizer has a built in tokenizer, we will use our custom function instead\n",
    "\n",
    "tokenisedChapters = [runTokenization(chapter) for chapter in preprocessedTextChapters]\n",
    "tokenisedChaptersWithoutStopwords = [removeStopWords(ChapterTokens) for ChapterTokens in tokenisedChapters]\n",
    "\n",
    "#Vectorise each chapter to make tf_idf vectors\n",
    "chapterVectors = GenerateTF_IDF_Vectors(tokenisedChaptersWithoutStopwords)\n",
    "print(\"TF_IDF matrix generated for the chapters:\")\n",
    "print(\" \")\n",
    "print(chapterVectors)\n",
    "\n",
    "#Calculating Similarity between chapters using cosine similiarity \n",
    "similarityScoresMatrix = ComputeSimilarityBetweenChapters(chapterVectors)\n",
    "print(\"Similarity matrix showing the similarity between chapters:\")\n",
    "print(\" \")\n",
    "print(similarityScoresMatrix)\n",
    "\n",
    "#visualise similarity using heat map\n",
    "VisualiseSimilarity(similarityScoresMatrix, tokenisedChaptersWithoutStopwords)\n",
    "\n",
    "\n",
    "# Example data (replace with your actual data)\n",
    "predicted_labels = ['ORDINAL', 'CARDINAL', 'DATE', 'GPE', 'ORG', 'GPE', 'DATE', 'CARDINAL', 'CARDINAL', 'CARDINAL', 'PERSON', 'PERSON', 'CARDINAL', 'CARDINAL', 'TIME', 'ORDINAL', 'PERSON', 'PERSON', 'PERSON', 'CARDINAL']\n",
    "manual_labels = ['ORDINAL', 'LOC', 'PERSON', 'DATE', 'LOC', 'GPE', 'DATE', 'PERSON', 'CARDINAL', 'CARDINAL', 'GPE', 'GPE', 'CARDINAL', 'CARDINAL', 'TIME', 'ORDINAL', 'PERSON', 'PERSON', 'PERSON', 'CARDINAL']\n",
    "\n",
    "\n",
    "# part-1 below\n",
    "performNERonEntireText(preprocessedText)\n",
    "\n",
    "#iteration-1\n",
    "passage1 = preprocessedText[0: 1000]\n",
    "passage2 = preprocessedText[2000: 3000]\n",
    "passage3 = preprocessedText[4000: 5000]\n",
    "passage = passage1 + passage2 + passage3\n",
    "predicted_labels = extractNERTagsFromRandomPassage(passage)\n",
    "print(predicted_labels)\n",
    "manual_labels = ['ORDINAL', 'LOC', 'PERSON', 'DATE', 'LOC', 'GPE', 'DATE', 'PERSON', 'CARDINAL', 'CARDINAL', 'GPE', 'GPE', 'CARDINAL', 'CARDINAL', 'TIME', 'ORDINAL', 'PERSON', 'PERSON', 'PERSON', 'PERSON']\n",
    "evaluatePerformance(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingConfusionMatrix(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingBarPlots(predicted_labels, manual_labels)\n",
    "\n",
    "\n",
    "#iteration-2\n",
    "passage4 = preprocessedText[6000: 7000]\n",
    "passage5 = preprocessedText[8000: 9000]\n",
    "passage6 = preprocessedText[10000: 11000]\n",
    "passage = passage4 + passage5 + passage6\n",
    "predicted_labels = extractNERTagsFromRandomPassage(passage)\n",
    "print(predicted_labels)\n",
    "\n",
    "manual_labels = ['PERSON', 'PERSON', 'PERSON', 'TIME', 'PERSON', 'DATE', 'PERSON', 'CARDINAL', 'PERSON', 'PERSON', 'GPE', 'ORDINAL', 'CARDINAL', 'CARDINAL', 'PERSON', 'ORG', 'TIME', 'MONEY']\n",
    "evaluatePerformance(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingConfusionMatrix(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingBarPlots(predicted_labels, manual_labels)\n",
    "\n",
    "\n",
    "#iteration-3\n",
    "passage7 = preprocessedText[12000: 13000]\n",
    "passage8 = preprocessedText[14000: 15000]\n",
    "passage9 = preprocessedText[16000: 17000]\n",
    "passage = passage7 + passage8 + passage9\n",
    "predicted_labels = extractNERTagsFromRandomPassage(passage)\n",
    "print(predicted_labels)\n",
    "manual_labels = ['PERSON', 'TIME', 'PERSON', 'PERSON', 'ORG', 'PERSON', 'PERSON', 'CARDINAL', 'EVENT']\n",
    "evaluatePerformance(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingConfusionMatrix(predicted_labels, manual_labels)\n",
    "visualisePerformanceUsingBarPlots(predicted_labels, manual_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
